{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Device name(s):\", [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())])\n",
    "# Get Slurm-allocated CPU count or fallback\n",
    "n_jobs = int(os.environ.get(\"SLURM_CPUS_PER_TASK\", os.cpu_count()))\n",
    "print(\"Number of jobs:\", n_jobs)\n",
    "n_human = int(n_jobs/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'model': 'lorenz',\n",
    "    'case': 'synthetic_lorenz',\n",
    "    'input_dim': 80,\n",
    "    'latent_dim': 3,\n",
    "    'poly_order': 2,\n",
    "    'include_sine': False,\n",
    "    'fix_coefs': False,\n",
    "    'svd_dim': 40,\n",
    "    'delay_embedded': True,\n",
    "    'scale': True,\n",
    "    'coefficient_initialization': 'constant',\n",
    "    \"coefficient_initial_value\": 1e-2,\n",
    "    'widths_ratios': [1.0, 0.6],\n",
    "\n",
    "    # Training\n",
    "    'max_epochs': 500,\n",
    "    'patience': 20,\n",
    "    'batch_size': 2**9,\n",
    "    'learning_rate': 1e-3,\n",
    "    \"lr_decay\": 0.999,\n",
    "\n",
    "    # Loss Weights\n",
    "    'loss_weight_rec': 0.3,\n",
    "    'loss_weight_sindy_z': 0.001,\n",
    "    'loss_weight_sindy_x': 0.001,\n",
    "    'loss_weight_sindy_regularization': 1e-5,\n",
    "    'loss_weight_integral': 0.1,\n",
    "    'loss_weight_x0': 0.01,\n",
    "    'loss_weight_layer_l2': 0.0,\n",
    "    'loss_weight_layer_l1': 0.0,\n",
    "\n",
    "    # SINDy\n",
    "    'coefficient_threshold': 1e-5,\n",
    "    'threshold_frequency': 5,\n",
    "    'print_frequency': 10,\n",
    "    'sindy_pert': 0.0,\n",
    "    'ode_net': False,\n",
    "    'ode_net_widths': [1.5, 2.0],\n",
    "    'exact_features': True,\n",
    "    'use_bias': True,\n",
    "\n",
    "    # Misc\n",
    "    'train_ratio': 0.8,\n",
    "    'data_path': './trained_models',\n",
    "    'save_checkpoints': False,\n",
    "    'save_freq': 5,\n",
    "    'learning_rate_sched': False,\n",
    "    'use_sindycall': False,\n",
    "    'sparse_weighting': None,\n",
    "    'system_coefficients': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from TrainingCode.TrainingCode import TrainModel  # adjust import path if needed\n",
    "from ParamOptCode.DefaultParams import params\n",
    "from AdditionalFunctionsCode.SolverFunctions import SynthData\n",
    "from DataGenerationCode.DataGeneration import LorenzSystem\n",
    "\n",
    "# === SETUP ===\n",
    "params[\"max_epochs\"] = 500\n",
    "params[\"input_dim\"] = params[\"svd_dim\"]  # ensure consistent dimensions\n",
    "params[\"batch_size\"] = 2**9  # larger batch to leverage GPU better\n",
    "params[\"dt\"] = 0.01 \n",
    "\n",
    "# === Generate synthetic data ===\n",
    "lorenz_system = LorenzSystem()\n",
    "synth_data = SynthData(input_dim=params[\"input_dim\"])\n",
    "z0 = torch.tensor([1.0, 1.0, 1.0])\n",
    "synth_data.run_sim(tend=32, dt=0.01, z0=z0.numpy(), apply_svd=True, svd_dim=params[\"svd_dim\"])\n",
    "\n",
    "# === Choose device ===\n",
    "# Change to 'cpu' to benchmark that version\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")  # Uncomment to test CPU explicitly\n",
    "\n",
    "# === Train ===\n",
    "trainer = TrainModel(synth_data, params, device=device)\n",
    "\n",
    "start = time.time()\n",
    "trainer.fit()\n",
    "end = time.time()\n",
    "\n",
    "device_type = \"GPU\" if device.type == \"cuda\" else \"CPU\"\n",
    "print(f\"\\n=== Training on {device_type} ({device}) completed in {(end - start)/60:.2f} minutes ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params[\"svd_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete old database\n",
    "import os\n",
    "import torch\n",
    "os.remove(\"optuna_study.db\")  # or your full path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ParamOptCode.ParameterOptimizer import HyperparameterOptimizer\n",
    "from ParamOptCode.DefaultParams import params \n",
    "import time\n",
    "#os.remove('C:/Users/mehme/OneDrive/Desktop/All/Code/LorenzReclaim/optuna_study.db')\n",
    "start_time = time.time()\n",
    "device = torch.device(\"cpu\")\n",
    "device_type = \"GPU\" if device.type == \"cuda\" else \"CPU\"\n",
    "params[\"input_dim\"] = params[\"svd_dim\"]\n",
    "optimizer = HyperparameterOptimizer(n_trials=2, n_jobs=1)\n",
    "best = optimizer.run()\n",
    "end_time = time.time()\n",
    "print(f\"Best hyperparameters: {best}\")\n",
    "print(f\"Hyperparameter optimization completed in {(end_time - start_time)/60:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
