{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device count: 1\n",
      "Device name(s): ['NVIDIA GeForce RTX 3050 6GB Laptop GPU']\n",
      "Number of jobs: 16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Device name(s):\", [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())])\n",
    "# Get Slurm-allocated CPU count or fallback\n",
    "n_jobs = int(os.environ.get(\"SLURM_CPUS_PER_TASK\", os.cpu_count()))\n",
    "print(\"Number of jobs:\", n_jobs)\n",
    "n_human = int(n_jobs/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'model': 'lorenz',\n",
    "    'case': 'synthetic_lorenz',\n",
    "    'input_dim': 80,\n",
    "    'latent_dim': 3,\n",
    "    'poly_order': 2,\n",
    "    'include_sine': False,\n",
    "    'fix_coefs': False,\n",
    "    'svd_dim': 40,\n",
    "    'delay_embedded': True,\n",
    "    'scale': True,\n",
    "    'coefficient_initialization': 'constant',\n",
    "    \"coefficient_initial_value\": 1e-2,\n",
    "    'widths_ratios': [1.0, 0.6],\n",
    "\n",
    "    # Training\n",
    "    'max_epochs': 500,\n",
    "    'patience': 20,\n",
    "    'batch_size': 2**9,\n",
    "    'learning_rate': 1e-3,\n",
    "    \"lr_decay\": 0.999,\n",
    "\n",
    "    # Loss Weights\n",
    "    'loss_weight_rec': 0.3,\n",
    "    'loss_weight_sindy_z': 0.001,\n",
    "    'loss_weight_sindy_x': 0.001,\n",
    "    'loss_weight_sindy_regularization': 1e-5,\n",
    "    'loss_weight_integral': 0.1,\n",
    "    'loss_weight_x0': 0.01,\n",
    "    'loss_weight_layer_l2': 0.0,\n",
    "    'loss_weight_layer_l1': 0.0,\n",
    "\n",
    "    # SINDy\n",
    "    'coefficient_threshold': 1e-5,\n",
    "    'threshold_frequency': 5,\n",
    "    'print_frequency': 10,\n",
    "    'sindy_pert': 0.0,\n",
    "    'ode_net': False,\n",
    "    'ode_net_widths': [1.5, 2.0],\n",
    "    'exact_features': True,\n",
    "    'use_bias': True,\n",
    "\n",
    "    # Misc\n",
    "    'train_ratio': 0.8,\n",
    "    'data_path': './trained_models',\n",
    "    'save_checkpoints': False,\n",
    "    'save_freq': 5,\n",
    "    'learning_rate_sched': False,\n",
    "    'use_sindycall': False,\n",
    "    'sparse_weighting': None,\n",
    "    'system_coefficients': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from TrainingCode.TrainingCode import TrainModel  # adjust import path if needed\n",
    "from ParamOptCode.DefaultParams import params\n",
    "from AdditionalFunctionsCode.SolverFunctions import SynthData\n",
    "from DataGenerationCode.DataGeneration import LorenzSystem\n",
    "\n",
    "# === SETUP ===\n",
    "params[\"max_epochs\"] = 500\n",
    "params[\"input_dim\"] = params[\"svd_dim\"]  # ensure consistent dimensions\n",
    "params[\"batch_size\"] = 2**9  # larger batch to leverage GPU better\n",
    "params[\"dt\"] = 0.01 \n",
    "\n",
    "# === Generate synthetic data ===\n",
    "lorenz_system = LorenzSystem()\n",
    "synth_data = SynthData(input_dim=params[\"input_dim\"])\n",
    "z0 = torch.tensor([1.0, 1.0, 1.0])\n",
    "synth_data.run_sim(tend=32, dt=0.01, z0=z0.numpy(), apply_svd=True, svd_dim=params[\"svd_dim\"])\n",
    "\n",
    "# === Choose device ===\n",
    "# Change to 'cpu' to benchmark that version\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")  # Uncomment to test CPU explicitly\n",
    "\n",
    "# === Train ===\n",
    "trainer = TrainModel(synth_data, params, device=device)\n",
    "\n",
    "start = time.time()\n",
    "trainer.fit()\n",
    "end = time.time()\n",
    "\n",
    "device_type = \"GPU\" if device.type == \"cuda\" else \"CPU\"\n",
    "print(f\"\\n=== Training on {device_type} ({device}) completed in {(end - start)/60:.2f} minutes ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params[\"svd_dim\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 15:22:26,698] Using an existing study with name 'sindy_opt' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Lorenz system...\n",
      "self.x (rec_v): (3120, 10)\n",
      "self.dx: (3120, 10)\n",
      "\n",
      "Active Loss Weights:\n",
      "loss_weight_rec: 0.6622743745527734\n",
      "loss_weight_sindy_z: 0.016239880477356985\n",
      "loss_weight_sindy_x: 0.02321062610095071\n",
      "loss_weight_sindy_regularization: 2.7619615198903407e-05\n",
      "loss_weight_integral: 0.014287323354111776\n",
      "loss_weight_x0: 0.0007395865706237733\n",
      "loss_weight_layer_l2: 0.006573024585874463\n",
      "loss_weight_layer_l1: 0.009866992557136803\n",
      "Epoch 0: Loss = 6.616799 | LR = 0.003076\n",
      "Epoch 10: Loss = 6.503344 | LR = 0.002853\n",
      "Epoch 20: Loss = 6.452836 | LR = 0.002645\n",
      "Epoch 30: Loss = 6.417363 | LR = 0.002453\n",
      "Epoch 40: Loss = 6.383078 | LR = 0.002275\n",
      "Epoch 50: Loss = 6.275696 | LR = 0.002110\n",
      "Epoch 60: Loss = 6.027059 | LR = 0.001956\n",
      "Epoch 70: Loss = 5.575470 | LR = 0.001814\n",
      "Epoch 80: Loss = 5.351608 | LR = 0.001682\n",
      "Epoch 90: Loss = 5.140810 | LR = 0.001560\n",
      "Epoch 100: Loss = 5.113349 | LR = 0.001447\n",
      "Epoch 110: Loss = 5.098843 | LR = 0.001342\n",
      "Epoch 120: Loss = 5.091484 | LR = 0.001244\n",
      "Epoch 130: Loss = 4.851310 | LR = 0.001154\n",
      "Epoch 140: Loss = 4.701530 | LR = 0.001070\n",
      "Epoch 150: Loss = 4.598560 | LR = 0.000992\n",
      "Epoch 160: Loss = 4.547020 | LR = 0.000920\n",
      "Epoch 170: Loss = 4.500469 | LR = 0.000853\n",
      "Epoch 180: Loss = 4.464057 | LR = 0.000791\n",
      "Epoch 190: Loss = 4.415389 | LR = 0.000734\n",
      "Epoch 200: Loss = 4.381640 | LR = 0.000680\n",
      "Epoch 210: Loss = 4.339647 | LR = 0.000631\n",
      "Epoch 220: Loss = 4.321561 | LR = 0.000585\n",
      "Epoch 230: Loss = 4.286511 | LR = 0.000543\n",
      "Epoch 240: Loss = 4.267464 | LR = 0.000503\n",
      "Epoch 250: Loss = 4.255011 | LR = 0.000467\n",
      "Epoch 260: Loss = 4.238663 | LR = 0.000433\n",
      "Epoch 270: Loss = 4.235669 | LR = 0.000401\n",
      "Epoch 280: Loss = 4.211196 | LR = 0.000372\n",
      "Epoch 290: Loss = 4.186938 | LR = 0.000345\n",
      "Epoch 300: Loss = 4.183949 | LR = 0.000320\n",
      "Epoch 310: Loss = 4.150134 | LR = 0.000297\n",
      "Epoch 320: Loss = 4.155869 | LR = 0.000275\n",
      "Epoch 330: Loss = 4.137616 | LR = 0.000255\n",
      "Epoch 340: Loss = 4.105315 | LR = 0.000237\n",
      "Epoch 350: Loss = 4.107783 | LR = 0.000219\n",
      "Epoch 360: Loss = 4.099753 | LR = 0.000203\n",
      "Epoch 370: Loss = 4.087509 | LR = 0.000189\n",
      "Epoch 380: Loss = 4.081985 | LR = 0.000175\n",
      "Epoch 390: Loss = 4.080702 | LR = 0.000162\n",
      "Epoch 400: Loss = 4.078113 | LR = 0.000150\n",
      "Epoch 410: Loss = 4.064291 | LR = 0.000140\n",
      "Epoch 420: Loss = 4.061626 | LR = 0.000129\n",
      "Epoch 430: Loss = 4.075836 | LR = 0.000120\n",
      "Epoch 440: Loss = 4.063218 | LR = 0.000111\n",
      "Epoch 450: Loss = 4.065064 | LR = 0.000103\n",
      "Epoch 460: Loss = 4.089739 | LR = 0.000096\n",
      "Epoch 470: Loss = 4.077977 | LR = 0.000089\n",
      "Epoch 480: Loss = 4.071675 | LR = 0.000082\n",
      "Epoch 490: Loss = 4.087076 | LR = 0.000076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-26 15:26:04,662] Trial 17 finished with value: 19.57076072692871 and parameters: {'loss_weight_rec': 0.6622743745527734, 'loss_weight_sindy_z': 0.016239880477356985, 'loss_weight_sindy_x': 0.02321062610095071, 'loss_weight_sindy_regularization': 2.7619615198903407e-05, 'loss_weight_integral': 0.014287323354111776, 'loss_weight_x0': 0.0007395865706237733, 'loss_weight_layer_l2': 0.006573024585874463, 'loss_weight_layer_l1': 0.009866992557136803, 'learning_rate': 0.003099528863125862, 'lr_decay': 0.9924839788755989, 'batch_size': 512, 'coefficient_threshold': 1.0146608312377015e-05}. Best is trial 17 with value: 19.57076072692871.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final SINDy Model:\n",
      "dz0/dt = -0.0819 * 1 + -0.0038 * z0 + 1.2147 * z1 + -1.0114 * z2 + -0.5789 * z0z0 + -0.4838 * z0z1 + 0.4089 * z0z2 + 1.4212 * z1z1 + -0.3229 * z1z2 + -1.5367 * z2z2\n",
      "dz1/dt = -0.4544 * 1 + -1.3458 * z0 + -1.1389 * z1 + 0.3821 * z2 + -0.4371 * z0z0 + -1.5701 * z0z1 + -1.9060 * z0z2 + -1.1196 * z1z1 + 0.4910 * z1z2 + 1.1011 * z2z2\n",
      "dz2/dt = 0.6766 * 1 + 1.4738 * z0 + -0.4503 * z1 + 1.2039 * z2 + 0.5049 * z0z0 + 1.9343 * z0z1 + 1.3843 * z0z2 + -0.9846 * z1z1 + -0.2789 * z1z2 + 1.3961 * z2z2\n",
      "\n",
      "Top 5 Trials by Static Loss:\n",
      "\n",
      "ðŸ“Œ Trial 1 (Global Trial #17):\n",
      "  Static Loss: 19.570761\n",
      "  Params:\n",
      "    loss_weight_rec: 0.6622743745527734\n",
      "    loss_weight_sindy_z: 0.016239880477356985\n",
      "    loss_weight_sindy_x: 0.02321062610095071\n",
      "    loss_weight_sindy_regularization: 2.7619615198903407e-05\n",
      "    loss_weight_integral: 0.014287323354111776\n",
      "    loss_weight_x0: 0.0007395865706237733\n",
      "    loss_weight_layer_l2: 0.006573024585874463\n",
      "    loss_weight_layer_l1: 0.009866992557136803\n",
      "    learning_rate: 0.003099528863125862\n",
      "    lr_decay: 0.9924839788755989\n",
      "    batch_size: 512\n",
      "    coefficient_threshold: 1.0146608312377015e-05\n",
      "\n",
      "  SINDy Model:\n",
      "    dz0/dt = -0.0819 * 1 + -0.0038 * z0 + 1.2147 * z1 + -1.0114 * z2 + -0.5789 * z0z0 + -0.4838 * z0z1 + 0.4089 * z0z2 + 1.4212 * z1z1 + -0.3229 * z1z2 + -1.5367 * z2z2\n",
      "    dz1/dt = -0.4544 * 1 + -1.3458 * z0 + -1.1389 * z1 + 0.3821 * z2 + -0.4371 * z0z0 + -1.5701 * z0z1 + -1.9060 * z0z2 + -1.1196 * z1z1 + 0.4910 * z1z2 + 1.1011 * z2z2\n",
      "    dz2/dt = 0.6766 * 1 + 1.4738 * z0 + -0.4503 * z1 + 1.2039 * z2 + 0.5049 * z0z0 + 1.9343 * z0z1 + 1.3843 * z0z2 + -0.9846 * z1z1 + -0.2789 * z1z2 + 1.3961 * z2z2\n",
      "\n",
      "ðŸ“Œ Trial 2 (Global Trial #15):\n",
      "  Static Loss: 20.399801\n",
      "  Params:\n",
      "    loss_weight_rec: 0.8416963665426982\n",
      "    loss_weight_sindy_z: 0.052073077852308594\n",
      "    loss_weight_sindy_x: 0.09516267912949403\n",
      "    loss_weight_sindy_regularization: 3.559956354107943e-08\n",
      "    loss_weight_integral: 0.0011560257561296762\n",
      "    loss_weight_x0: 0.04091371318039725\n",
      "    loss_weight_layer_l2: 9.527988576816561e-05\n",
      "    loss_weight_layer_l1: 0.006258730581444608\n",
      "    learning_rate: 0.001931811028851417\n",
      "    lr_decay: 0.9999760342950846\n",
      "    batch_size: 512\n",
      "    coefficient_threshold: 2.5399943969196324e-06\n",
      "\n",
      "  SINDy Model:\n",
      "    dz0/dt = 0.0812 * 1 + -0.0703 * z0 + 0.8445 * z1 + -1.2236 * z2 + -0.7315 * z0z0 + 0.2602 * z0z1 + 0.0506 * z0z2 + 1.1873 * z1z1 + -1.2463 * z1z2 + -1.2739 * z2z2\n",
      "    dz1/dt = -0.1837 * 1 + -1.0028 * z0 + -0.6362 * z1 + 1.0371 * z2 + -0.2576 * z0z0 + -1.3336 * z0z1 + -1.0374 * z0z2 + -0.7844 * z1z1 + 1.2107 * z1z2 + 0.8575 * z2z2\n",
      "    dz2/dt = 0.4781 * 1 + 1.4606 * z0 + 0.1469 * z1 + 0.0813 * z2 + 0.7099 * z0z0 + 1.6429 * z0z1 + 1.4653 * z0z2 + -0.2236 * z1z1 + -0.5774 * z1z2 + 0.4810 * z2z2\n",
      "\n",
      "ðŸ“Œ Trial 3 (Global Trial #6):\n",
      "  Static Loss: 24.920681\n",
      "  Params:\n",
      "    loss_weight_rec: 0.0158713650425662\n",
      "    loss_weight_sindy_z: 0.012033735077702912\n",
      "    loss_weight_sindy_x: 0.010926640286170445\n",
      "    loss_weight_sindy_regularization: 4.82879390268073e-07\n",
      "    loss_weight_integral: 0.0026293566767638505\n",
      "    loss_weight_x0: 0.0012057599419958358\n",
      "    loss_weight_layer_l2: 0.002585559962611924\n",
      "    loss_weight_layer_l1: 0.0007692030711144626\n",
      "    learning_rate: 0.002389263071831499\n",
      "    lr_decay: 0.9910181533583896\n",
      "    batch_size: 512\n",
      "    coefficient_threshold: 2.4832018426087454e-05\n",
      "\n",
      "  SINDy Model:\n",
      "    dz0/dt = -0.4861 * 1 + 1.2091 * z0 + -0.1228 * z1 + -0.5641 * z2 + 1.4310 * z0z0 + -1.3486 * z0z1 + 1.3907 * z0z2 + 0.8048 * z1z1 + -0.1945 * z1z2 + -0.6146 * z2z2\n",
      "    dz1/dt = -0.1185 * 1 + 1.2566 * z0 + -0.1747 * z1 + -0.3913 * z2 + 1.0675 * z0z0 + -1.2611 * z0z1 + 1.4776 * z0z2 + 0.8283 * z1z1 + -0.0326 * z1z2 + -0.6391 * z2z2\n",
      "    dz2/dt = 0.5380 * 1 + -0.9155 * z0 + 0.0454 * z1 + 0.4711 * z2 + -1.1207 * z0z0 + 1.0753 * z0z1 + -1.1617 * z0z2 + -0.5737 * z1z1 + 0.2139 * z1z2 + 0.4631 * z2z2\n",
      "\n",
      "ðŸ“Œ Trial 4 (Global Trial #14):\n",
      "  Static Loss: 25.747311\n",
      "  Params:\n",
      "    loss_weight_rec: 0.034567548416201445\n",
      "    loss_weight_sindy_z: 0.08373028334231071\n",
      "    loss_weight_sindy_x: 0.07657331460511815\n",
      "    loss_weight_sindy_regularization: 2.4530132706705013e-08\n",
      "    loss_weight_integral: 0.0010145215253245755\n",
      "    loss_weight_x0: 0.022196891700625134\n",
      "    loss_weight_layer_l2: 0.0007087785934561689\n",
      "    loss_weight_layer_l1: 0.0061898902166607084\n",
      "    learning_rate: 0.001849313587254913\n",
      "    lr_decay: 0.9924228323654394\n",
      "    batch_size: 512\n",
      "    coefficient_threshold: 2.9128879678229393e-06\n",
      "\n",
      "  SINDy Model:\n",
      "    dz0/dt = -0.0561 * 1 + -0.0262 * z0 + 0.5397 * z1 + -0.7969 * z2 + -0.0287 * z0z0 + 0.5269 * z0z1 + -0.8340 * z0z2 + -0.7870 * z1z1 + 0.7453 * z1z2 + 1.2412 * z2z2\n",
      "    dz1/dt = -0.0493 * 1 + -0.1341 * z0 + 0.4638 * z1 + -0.6858 * z2 + -0.1186 * z0z0 + 0.5856 * z0z1 + -0.6885 * z0z2 + -0.6737 * z1z1 + 0.7046 * z1z2 + 1.0821 * z2z2\n",
      "    dz2/dt = -0.0667 * 1 + 0.0822 * z0 + 0.5271 * z1 + -0.7396 * z2 + 0.1368 * z0z0 + 0.4581 * z0z1 + -0.8009 * z0z2 + -0.7598 * z1z1 + 0.6597 * z1z2 + 1.0004 * z2z2\n",
      "\n",
      "ðŸ“Œ Trial 5 (Global Trial #5):\n",
      "  Static Loss: 26.170088\n",
      "  Params:\n",
      "    loss_weight_rec: 0.3454799928030793\n",
      "    loss_weight_sindy_z: 0.08161696368411551\n",
      "    loss_weight_sindy_x: 0.004818085668425073\n",
      "    loss_weight_sindy_regularization: 0.0006867274137802603\n",
      "    loss_weight_integral: 0.005365991910898771\n",
      "    loss_weight_x0: 0.009586474207766966\n",
      "    loss_weight_layer_l2: 0.0030094714805471512\n",
      "    loss_weight_layer_l1: 0.003864694572297857\n",
      "    learning_rate: 0.0013336881173117512\n",
      "    lr_decay: 0.9997464886082363\n",
      "    batch_size: 1024\n",
      "    coefficient_threshold: 4.946135263287602e-06\n",
      "\n",
      "  SINDy Model:\n",
      "    dz0/dt = 0.9236 * 1 + -0.4911 * z0 + -1.7928 * z1 + -1.8067 * z2 + 0.3107 * z0z0 + -0.1351 * z0z1 + -0.0172 * z0z2 + -1.6367 * z1z1 + -1.8497 * z1z2 + -1.9075 * z2z2\n",
      "    dz1/dt = -0.8551 * 1 + 1.6237 * z0 + 0.8862 * z1 + 1.7741 * z2 + -0.2635 * z0z0 + 1.3610 * z0z1 + 1.2961 * z0z2 + 0.4568 * z1z1 + 1.3125 * z1z2 + 1.7535 * z2z2\n",
      "    dz2/dt = 0.3372 * 1 + 1.7714 * z0 + -1.6248 * z1 + -0.5711 * z2 + 0.5888 * z0z0 + 1.0069 * z0z1 + 1.7414 * z0z2 + -1.5386 * z1z1 + -1.5576 * z1z2 + 0.1694 * z2z2\n",
      "Best hyperparameters: FrozenTrial(number=17, state=1, values=[19.57076072692871], datetime_start=datetime.datetime(2025, 4, 26, 15, 22, 26, 717510), datetime_complete=datetime.datetime(2025, 4, 26, 15, 26, 4, 645373), params={'loss_weight_rec': 0.6622743745527734, 'loss_weight_sindy_z': 0.016239880477356985, 'loss_weight_sindy_x': 0.02321062610095071, 'loss_weight_sindy_regularization': 2.7619615198903407e-05, 'loss_weight_integral': 0.014287323354111776, 'loss_weight_x0': 0.0007395865706237733, 'loss_weight_layer_l2': 0.006573024585874463, 'loss_weight_layer_l1': 0.009866992557136803, 'learning_rate': 0.003099528863125862, 'lr_decay': 0.9924839788755989, 'batch_size': 512, 'coefficient_threshold': 1.0146608312377015e-05}, user_attrs={'latent_dim': 3, 'poly_order': 2, 'sindy_coefficients': [[-0.08188041299581528, -0.4543652832508087, 0.6766353845596313], [-0.0038049588911235332, -1.3457962274551392, 1.4738373756408691], [1.2146668434143066, -1.138914704322815, -0.4503200054168701], [-1.0113908052444458, 0.3820965588092804, 1.2039164304733276], [-0.578859269618988, -0.43708932399749756, 0.5049479603767395], [-0.4837890565395355, -1.5700905323028564, 1.93427312374115], [0.4089277684688568, -1.906026840209961, 1.3842518329620361], [1.4211583137512207, -1.1195988655090332, -0.9845537543296814], [-0.32294654846191406, 0.4909754991531372, -0.27888545393943787], [-1.536744475364685, 1.1011043787002563, 1.3961464166641235]]}, system_attrs={}, intermediate_values={}, distributions={'loss_weight_rec': FloatDistribution(high=1.0, log=True, low=0.001, step=None), 'loss_weight_sindy_z': FloatDistribution(high=0.1, log=True, low=1e-06, step=None), 'loss_weight_sindy_x': FloatDistribution(high=0.1, log=True, low=1e-06, step=None), 'loss_weight_sindy_regularization': FloatDistribution(high=0.01, log=True, low=1e-08, step=None), 'loss_weight_integral': FloatDistribution(high=1.0, log=True, low=0.001, step=None), 'loss_weight_x0': FloatDistribution(high=1.0, log=True, low=0.0001, step=None), 'loss_weight_layer_l2': FloatDistribution(high=0.01, log=False, low=0.0, step=None), 'loss_weight_layer_l1': FloatDistribution(high=0.01, log=False, low=0.0, step=None), 'learning_rate': FloatDistribution(high=0.01, log=True, low=1e-05, step=None), 'lr_decay': FloatDistribution(high=1.0, log=False, low=0.99, step=None), 'batch_size': CategoricalDistribution(choices=(128, 256, 512, 1024)), 'coefficient_threshold': FloatDistribution(high=0.001, log=True, low=1e-06, step=None)}, trial_id=18, value=None)\n",
      "Hyperparameter optimization completed in 3.69 minutes\n"
     ]
    }
   ],
   "source": [
    "from ParamOptCode.ParameterOptimizer import HyperparameterOptimizer\n",
    "from ParamOptCode.DefaultParams import params \n",
    "import time\n",
    "#os.remove('C:/Users/mehme/OneDrive/Desktop/All/Code/LorenzReclaim/optuna_study.db')\n",
    "start_time = time.time()\n",
    "device = torch.device(\"cpu\")\n",
    "device_type = \"GPU\" if device.type == \"cuda\" else \"CPU\"\n",
    "params[\"input_dim\"] = params[\"svd_dim\"]\n",
    "optimizer = HyperparameterOptimizer(n_trials=1, n_jobs=1)\n",
    "best = optimizer.run()\n",
    "end_time = time.time()\n",
    "print(f\"Best hyperparameters: {best}\")\n",
    "print(f\"Hyperparameter optimization completed in {(end_time - start_time)/60:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
